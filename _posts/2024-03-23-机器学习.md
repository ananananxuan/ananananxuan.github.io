---
layout: mypost
title: 机器学习
categories: [机器学习]
extMath: true
---
# 机器学习

机器学习分为有监督学习和无监督学习

## 1.有监督学习：特征值＋目标值
分类/回归  

分类算法：K近邻、决策树、随机森林、支持向量机、朴素贝叶斯、逻辑回归、XGBoost、LightGBM、神经网络

回归算法：线性回归、岭回归、Lasso回归、ElasticNet回归、XGBoost回归、LightGBM回归、神经网络

## 2.无监督学习：特征值
聚类  

# 机器学习流程
机器学习的一般步骤  
1. 获取数据 sklearn自带数据集、kaggle  
2. 数据处理 缺失值/不符合要求的数据/过滤低频数据/年月日/分类特征转换  pandas
3. 数据划分
4. 特征工程 标准化归一化，降维（按需），one-hot编码，特征提取（文本，图片）
5. 机器学习算法训练  
6. 模型选择与调优
7. 模型评估   
8. 模型保存  
 
## 1.获取数据（sklearn自带数据集、kaggle）
sklearn.datasets.load_* 小规模数据集    
sklearn.datasets.fetch_* 大规模数据集    

包括feature_names，data和target_names，target 

```
from sklearn.datasets import load_iris    
iris = load_iris()    

print(iris.data) X  
print(iris.target) y  
print(iris.feature_names) feature_names  
print(iris.target_names) target_names  
```

## 3.数据划分

数据划分  
1.划分训练集和测试集  
2.划分特征和目标值 
```
from sklearn.model_selection import train_test_split  
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=22)
```

## 4.特征工程 

### 1.字典特征  
将类别转化成one-hot 编码

```
from sklearn.feature_extraction import DictVectorizer

dv = DictVectorizer(sparse=False )
sparse = False 表示返回的值不是稀疏矩阵
sparse = True 表示返回的值是稀疏矩阵 可以节省内存

data = [{'city': '北京', 'temperature': 100}, {'city': '上海', 'temperature': 60}]   

X = dv.fit_transform(data)  

print(dv.get_feature_names())  
print(X)  
```
### 2.文本特征提取   

countvectorizer统计特征词个数  
tfidfvectorizer统计特征词重要性   


countervectorizer 

①对英文进行文本抽取
```
from sklearn.feature_extraction.text import CountVectorizer

transfer = CountVectorizer(stop_words=['is']) 
stop_words 停用词，可以在网上找停用词表
data = [
    "life is short, i like python",
    "life is too long, i dislike python",
    "i want to write some python code"
]

X = transfer.fit_transform(data)

print(transfer.get_feature_names())  
print(X.toarray())
```

②对中文进行文本抽取  
1）中文划词  
```
import jieba

data = [
    "我爱北京天安门"
]
a = "".join(list(jieba.cut(data, cut_all=False)))
print(a)
```
2）划词提取关键字  
```
data2 = [
    "我爱北京天安门",
    "天安门上太阳升"
]

data_new = []
for sentence in data2:
    a = " ".join(list(jieba.cut(sentence, cut_all=False)))
    data_new.append(a)

print(data_new)

from sklearn.feature_extraction.text import CountVectorizer

transfer = CountVectorizer()
X = transfer.fit_transform(data_new)

print(transfer.get_feature_names())  
print(X.toarray())
```

**tfidfvectorizer**  
TF term frequency 词频    
IDF inverse document frequency 逆向文件频率      
```
    from sklearn.feature_extraction.text import TfidfVectorizer

    transfer = TfidfVectorizer()
    X = transfer.fit_transform(data_new)

    print(transfer.get_feature_names())
    print(X.toarray())
```
### 3.特征预处理
①归一化
因为每个特征的量纲不统一，所以要做归一化，映射到[0,1]之间,但是如果最大值/最小值出现异常值，容易受到影响，适合精确的小数据场景。

x1 = x-min/max-min  
x2 = x1*(mx-mi)+mi
```
from sklearn.preprocessing import MinMaxScaler

transfer = MinMaxScaler()

X = transfer.fit_transform(data)

print(X)
```

②标准化 
对每个特征的每个样本进行标准化，适合大样本场景。  
x = x - mean/std   
转换成均值为0，方差为1的正态分布  
```
from sklearn.preprocessing import StandardScaler

transfer = StandardScaler()

X = transfer.fit_transform(data)

print(X)
```
### 4.特征降维
对二维数组进行降维，降低特征个数，得到一组不相关主变量的过程，减少冗余信息   
①特征选择  
*filter 过滤式* ，过滤低方差特征  
```
from sklearn.feature_selection import VarianceThreshold

transfer = VarianceThreshold(threshold=0.0)

X = transfer.fit_transform(iris.data)

print(X)
```
    
*相关系数选择 (皮尔森相关系数选择)* 
[-1,1] >0正相关 <0负相关  
绝对值<0.4 低相关  
绝对值>0.7 高相关  

```
from scipy.stats import pearsonr
personr(x,y)计算x和y的相关性
```
当遇到特征相关性强的，选取其中一个特征或者加权求和  

②主成分分析降维
```
from sklearn.decomposition import PCA
transfer = PCA(n_components=2) #表示降到2维，传小数表示保留信息0.95
X = transfer.fit_transform(data)
print(X)
```
### 5.转换器

转换器(特征工程的父类)    
实例化，将转换器类进行实例化 transformer  
调用fit_transform方法  
eg:标准化  x-mean/std  
```
standardscaler = StandardScaler()
X = standardscaler.fit_transform(X)
```
fit()用于计算每一列的均值和方差
transform()用于带入x-mean/std

## 5.机器学习算法

### 估计器(机器学习算法的父类)  
实例化，将估计器类进行实例化 estimator  
调用，estimator.fit(X_train,y_train)  
模型评估,直接比较预测值和真实值
```
y_predict = estimator.predict(X_test)
y_test == y_predict
```
计算准确率
accuracy = estimator.score(X_test,y_test)




# 1.KNN算法
  根据邻居距离预测类别，需要无量纲化（标准化）  

  k表示邻居的个数    
  k过大容易受到样本不均衡的影响  
  k过小容易受到异常值影响  

  确认距离的方法  
  欧式距离：(x1-x2)^2+(y1-y2)^2+(z1-z2)^2 
  曼哈顿距离：|x1-x2|+|y1-y2|    
  明科夫斯基：(|x1-x2|^p+|y1-y2|^p)^(1/p)

  knn优点：简单，易于理解  
  knn缺点：每个距离都要算，内存开销大，需要指定k值  

  使用场景：小数据样本，几千~几万

**模型选择与调优**

1.交叉验证（cross validation）   
 将训练集分为训练集和验证集  
 训练集用于训练模型，验证集用于评估模型  
 将数据分为3:1，经过n次测试，每一次都更换不同的验证集，得到n次模型的结果，去平均值作为最终结果，又称4折交叉验证  
 目的：为了让评估的模型更加准确可信  


2.网格搜索（grid search）    
遍历所有参数，找到最优参数  

```
from sklearn.model_selection import GridSearchCV

estimator = KNeighborsClassifier()

param_dict = {'n_neighbors':[3,5,7,9,11]}

estimator = GridSearchCV(estimator,param_dict,cv=5)

estimator.fit(X_train,y_train)

print(estimator.best_params_) #k的最佳取值
print(estimator.best_score_) #验证集的最佳得分
print(estimator.best_estimator_) 
print(estimator.cv_results_)
```

# 2.朴素贝叶斯

贝叶斯公式：
P(A|B) = P(B|A)P(A)/P(B)

朴素贝叶斯：朴素+贝叶斯（事件独立+贝叶斯）

优点：
1. 算法简单，易于实现，常用于文本分类
2. 准确率高，速度快，对缺失值不敏感

缺点：
词之间有相关性时，结果不准确

朴素贝叶斯算法适用场景：
文本分类

```
from sklearn.naive_bayes import MultinomialNB

estimator = MultinomialNB(alpha=1.0)
```

联合概率：
P(A,B) = P(A|B)P(B)

条件概率：
P(A|B) = P(A,B)/P(B)

相互独立：
P(A,B) = P(A)P(B)  

# 3.决策树
优点：简单高效决策，可视化，可解释能力强。  
缺点：树太大的话容易过拟合 

信息：消除随机不定性的东西

信息熵：总的不确定性
H(X) = -∑ [P(xi)log2P(xi)]

信息增益：知道某一个特征的信息之后对总的信息熵减少的程度，即知道某一个特征的信息之后，不确定性减少最多。

特征A对训练数据集D的信息增益G(D,A)

G(D,A) = H(D) - H(D|A)

```
from sklearn.tree import DecisionTreeClassifier

estimator = DecisionTreeClassifier(criterion='entropy/gini',max_depth=3,random_state=0)

estimator.fit(X_train,y_train)

y_predict = estimator.predict(X_test)

print(estimator.score(X_test,y_test))

```

树的可视化

```
  from sklearn.tree import export_graphviz
  export_graphviz(estimator,out_file='tree.dot',feature_names=iris.feature_names,class_names=iris.target_names,rounded=True,filled=True)
```
将dot文件内容拿到网站进行树的可视化
[http://www.webgraphviz.com/?source=techstories.org](http://www.webgraphviz.com/?source=techstories.org)  

决策树使用场景：数据量更大

# 4.随机森林
集成学习：将多个弱学习器组合成一个强学习器  
随机森林：包含多个决策树的集成学习算法 

适用场景：
1. 数据量很大，特征很多
2. 数据集存在样本不均衡，即样本不平衡
3. 数据集存在特征相关性

随机森林算法：
1. 训练集随机
    bootstrap：随机有放回抽样，组成新的训练集
2. 特征随机
    从M个特征中随机抽取m个特征，可以起到降维和加速度作用

```
from sklearn.ensemble import RandomForestClassifier

estimator = RandomForestClassifier()

网格搜索
param_dict = {"n_estimators":[100,500,800,1200],
                "max_depth":[None,5,8,10,15,20],
                "min_samples_split":[2,5,8,10],}
estimator = GridSearchCV(estimator,param_grid=param_dict,cv=5)
estimator.fit(X_train, y_train)
```

# 5.线性模型
线性回归，target是连续性的

目标:求参数，使得预测准确

公式：
y = w1*x1+w2*x2+w3*x3+b  
w叫做权重  
b叫做偏置  

线性关系：函数关系

线性模型：  
自变量一次：
y = w1*x1+w2*x2+w3*x3+b  
参数一次：（属于非线性关系）
y = w1*x1+w2*x2^2+w3*x3^3+b

线性关系一定是线性模型,线性模型不一定是线性关系

损失函数： 
预测值与真实值的差距

loss = (hw(x1)-y1)^2+(hw(x2)-y2)^2+(hw(x3)-y3)^2+...+(hw(xn)-yn)^2

最小二乘法：
求解使得损失函数最小的参数

优化使得损失函数最小：

**正规方程：**  
w = (X^TX)^-1X^Ty
直接求得最小值    
特征较多的时候不适用，不能解决拟合问题  
```
from sklearn.linear_model import LinearRegression

estimator = LinearRegression()

estimator.fit(X_train,y_train)

print(estimator.coef_) #权重
print(estimator.intercept_) #偏置

```


**梯度下降法：**  
1. 随机初始化参数
2. 计算损失函数
3. 计算损失函数对参数的偏导数
4. 更新参数
适用于数据量较大

```
from sklearn.linear_model import SGDRegressor

estimator = SGDRegressor(eta0=0.01,max_iter=1000,random_state=0)
# eta表示学习率，max_iter表示迭代次数

estimator.fit(X_train,y_train)  

print(estimator.coef_) #权重
print(estimator.intercept_) #偏置
```
梯度下降的优化方法
1. 随机梯度下降法（SGD）
2. 随机平均梯度下降法（SAG）
3. 小批量梯度下降法（mini-batch）

**回归性能评估**  
均方误差：Mean Squared Error，越小越好  
MSE = (1/n) * ∑(hw(xn)-yn)^2

```
from sklearn.metrics import mean_squared_error

y_predict = estimator.predict(X_test)

print(mean_squared_error(y_test,y_predict))
  
```

**过拟合与欠拟合**  
1.过拟合：    
模型过于复杂，对训练集拟合的很好，但是对测试集拟合的不好,模型泛化能力比较差   
解决方法：  
正则化L1/L2，损失函数+惩罚项lambda∑W^2  

L1:损失函数+惩罚项lambda∑|W|，可以让一些权重接近于0，删除某个特征的影响 LASSO   
L2:损失函数+惩罚项lambda∑W^2，可以让一些权重接近于0，削弱某个特征的影响 Ridge-岭回归  
```
from sklearn.linear_model import Ridge

estimator = Ridge(alpha=1.0)
# alpha表示正则化力度,惩罚项系数

estimator.fit(X_train,y_train)

print(estimator.coef_) #权重

```
正则化力度越大，权重系数越小  
正则化力度越小，权重系数越大  



2.欠拟合：  
模型过于简单，对训练集拟合的不好，对测试集拟合的也不好  
解决方法： 
增加数据和特征

# 6.逻辑回归
逻辑回归是线性模型，但是target是离散的，属于二分类问题  
通过将线性的输出映射到sigmoid函数上(0,1)。设置阈值，＞阈值为1，＜阈值为0  

sigmoid函数：  
y = 1/(1+e^(-z)) 阈值(0,1)

损失函数：  
-log(hw(x)) 真实值=1  
-log(1-hw(x)) 真实值=0  
合起来：
-log(hw(x))+log(1-hw(x))

通过梯度下降来优化参数

```
from sklearn.linear_model import LogisticRegression

estimator = LogisticRegression()

estimator.fit(X_train,y_train) 

print(estimator.coef_) #权重
print(estimator.intercept_) #偏置
```

# 模型评估

参见混淆矩阵
![混淆矩阵](matrix.png)

1.准确率：    
预测正确的样本数/总样本数    
accuracy = TP+TN/(TP+TN+FP+FN)  

2.精确率： 
预测为正样本中实际为正样本的比例  
precision = TP/(TP+FP)  


3.召回率：反映查的全不全  
实际为正样本中预测为正样本的比例  
（真的患癌症了能够被检查出来的概率/查次品）  
recall = TP/(TP+FN)  


4.F1值：
综合考虑精确率和召回率

F1 = 2*(precision*recall)/(precision+recall)


```
from sklearn.metrics import classification_report

print(classification_report(y_test,y_predict,labels=[0,1],target_names=['no','yes']))
```
注：当样本分类不平衡的时候，matrix不好用

5.ROC：  
TPR=TP/(TP+FN) 召回率  
FPR=FP/(FP+TN) 假正率  
![ROC曲线](ROC.png)

ROC曲线和x轴y轴围成的面积叫做AUC  
AUC=0.5 随机猜测   
AUC=1 完美预测  
AUC只能用来评价二分类，适合样本分布不平衡的情况  
公式：AUC = 1/2*(1+∑(TPRi-FPRi))  


 ```
 from sklearn.metrics import roc_auc_score

 print(roc_auc_score(y_test,y_predict))
#需要y_test,y_predict是0/1

 ```

# 模型保存和加载
 
 ```
   from sklearn.externals import joblib

   joblib.dump(estimator,'estimator.pkl')#保存
   estimator = joblib.load('estimator.pkl')#加载
 ```

# 无监督学习
 1. 聚类 k-means 在没有目标值的时候使用
 2. 降维 PCA 特征过多

K-means算法：
1. 随机初始化k个中心点
2. 计算每个样本点到中心点的距离，将样本点分配到最近的中心点
3. 更新中心点的位置，即计算每个中心点对应的样本点的均值
4. 重复2,3步骤，直到中心点不再移动（迭代）

```
from sklearn.cluster import KMeans

estimator = KMeans(n_clusters=3,random_state=0)

estimator.fit(X)

print(estimator.labels_) #每个样本点对应的簇的标签
print(estimator.cluster_centers_) #每个簇的中心点

```
  
聚类效果评价：  
高内聚，低耦合  
轮廓系数 = bi - ai / max(bi - ai)  
bi:样本点到其他簇的点的距离最小值  
ai:样本点到自己簇的距离平均值  
越接近1，聚类效果越好  

```
from sklearn.metrics import silhouette_score

print(silhouette_score(X,y_predict))
```
  





 






  





 

