---
layout: mypost
title: 损失函数
categories: [深度学习]
extMath: true
---


## 损失函数  

### 1.定义
损失函数（Loss function）是用于评估模型预测与真实值差距的度量。

### 2.作用
损失函数是用来评估模型预测结果与真实结果的差异性，从而指导模型进行训练。

### 3.常见的损失函数

#### 1.均方误差（MSE）
最为常见的损失函数，主用于回归问题。MSE计算的是预测值与真实值之间差值的平方的平均值。

![MSE](MSE.png)

```
import torch
import torch.nn as nn

loss = nn.MSELoss()
# 假设input和target分别为预测值和真实值
loss_value = loss(input, target)
``` 


#### 2.交叉熵损失函数（Cross Entropy Loss）

交叉熵损失主要用于分类问题，尤其是在二分类（如Sigmoid交叉熵）和多分类（如Softmax交叉熵）任务中。它对预测概率分布与实际概率分布之间的差距进行衡量。交叉熵损失nn.CrossEntropyLoss()在内部执行了Log Softmax操作，因此，它的输入应当是未经Softmax的原始分数（logits）。


```
    loss = nn.CrossEntropyLoss()
    # input是一个矩阵，每行表示一个样本的预测类别分数，
    # target是一个向量，表示每个样本的真实类别
    loss_value = loss(input, target)
```


#### 3.平均绝对误差（MAE）

与MSE类似，MAE计算的是预测值与真实值的绝对差的平均值。相比于MSE，MAE对异常值具有更强的鲁棒性。

```
loss = nn.L1Loss()
loss_value = loss(input, target)
```


#### 4.Huber损失函数

Huber损失是均方误差和平均绝对误差的结合。它在预测误差较小时表现如MSE，在预测误差较大时表现如MAE，兼具两者的优点。

```
loss = nn.SmoothL1Loss()
loss_value = loss(input, target)
```

#### 5.对数损失

对数损失是交叉熵损失的另一种常见形式，主要用于二分类问题。它考虑的是预测概率本身的对数值，而非简单的预测正确与否。在PyTorch中，可以使用BCELoss(Binary Cross Entropy Loss)替代对数损失，主要用于二分类问题。

```
    loss = nn.BCELoss()
    loss_value = loss(input, target)
```

#### 6.Hinge损失

Hinge损失常用于支持向量机（SVM）和一些感知机算法，但也可以用于深度学习模型中，特别是对于一些"最大-最小"类别判别的问题。

```
    loss = nn.HingeEmbeddingLoss()
    loss_value = loss(input, target)
```


