---
layout: mypost
title: NLP
categories: [深度学习]
extMath: true
---

### 1.自然语言处理数据

#### 1.1 文本数据

词与词，句子与句子，段落与段落之间有顺序

![数据样子](vector.png)
![数据样子](vectors.png)
在中文文字数据中，一张二维表是一个句子或者一段话，用词向量表示
(sequence_length,embedding_dimension)

(batch_size,sequence_length,embedding_dimension)

##### 分词
将一段文本分割成词语

1.中文分词   
jieba分词  
SNLP  
```
from gensim.models import Word2Vec
import jieba

sentence = list(jieba.cut("我爱北京天安门"))

sentences = [sentence]

print(sentences)

model = Word2Vec(sentences, vector_size=5,min_count=1,workers=4)

vector = model.wv['北京']
# [-0.14233617  0.12917745  0.17945977 -0.10030856 -0.07526743]
print(vector)
```
2.英文分词    
spacy分词  
BPE  



#### 1.2 时间序列数据

股票、气温记录、心电图

![数据样子](timeseries.png)

一张二维表单(time_step,feature_dimension)
多张二维表单(batch_size,time_step,feature_dimension)

#### 1.3 音频数据

#### 1.4 视频数据

……DNA序列数据
 
### 2.循环神经网络RNN
recurrent neural network
MLP处理方法
![MLP处理方法](rnn_word.png)
RNN处理方法
![RNN处理方法](rnn_word2.png)
![RNN处理方法](rnn_word3.png)

划分了几个词就进行几次正向传播，正向传播的次数称为时间步

将处理第一个字的隐藏层的信息传给处理下一个字的隐藏层

![RNN](RNN1.png)


rnn循环神经网络输入的是所有表的第一行
![deal](deal.png)

词嵌入 Embedding Layers


RNN网络： 

```
import torch
import torch.nn as nn

class RNN(nn.Module):
    def __init__(self,input_size,hidden_size,output_size):
        super(RNN,self).__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size,hidden_size) #input_size=input dimension,即词向量长度。hidden_size自己手动设置
        self.fc = nn.Linear(hidden_size,output_size)
    def forward(self,input,hidden):
        output,hidden = self.rnn(input,hidden)
        output = self.fc(output[-1])
        return output,hidden
    def init_hidden(self,batch_size):
        return torch.zeros(1,batch_size,self.hidden_size)
```
        
```
input = torch.randn(10,1,10)  # 输入的shape(seq_len,batch_size,input_size)
hidden = model.init_hidden(1)
output,hidden = model(input,hidden)
print(output)   #输出的shape(seq_len,batch_size,hidden_size)  每一个时间步的最后一个隐藏层输出状态的集合 天气预报
print(hidden)  #输出的shape(num_layers,batch_size,hidden_size) 最后一个循环层上所有隐藏层的状态  句子的情感
```
    
RNN网络的缺点：
所有信息照单全收，记忆机制中毫无重点，长期记忆容易被遗忘，所以很难处理较长的序列
容易出现梯度消失和梯度爆炸

### 3.长短期记忆网络LSTM
在RNN的基础上，知道哪些是重点，在重点词语上设置更大的权重，避免遗忘

该网络需要自行选择吸收多少新信息，遗忘多少新信息，判断对当前时间步的预测来说，最重要的信息是哪些，并将该信息输出给当前时间步

![RNN和LSTM的区别](diff_RNN_LSTM.png)

LSTM和RNN区别在于多了一个记忆细胞

其中h负责短期信息的记忆，c负责长期信息的记忆

![LSTM](doors.png)

遗忘门决定忘记哪些旧知识，输入门决定记住哪些新知识，输出门决定根据需要输出哪些知识

![cell](cell.png)

```
import torch
import torch.nn as nn

class LSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=1, output_size=1):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

        
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        out, hn, cn = self.lstm(x, (h0, c0))
        out = self.fc(out[:, :, :])
        return out
```

nn.LSTM(input_size,num_layers,hidden_size, dropout=0,batch_first=False)  



input_size=input dimension,即词向量长度。hidden_size自己手动设置, num_layers自己手动设置隐藏层的层数,dropout= [0,1),在隐藏层之间随机丢弃一些神经元，以减少过拟合。batch_first=True,输入数据的维度是(batch,seq,feature) batch_first=False,输入数据的维度是(seq,batch,feature)


```
        input = torch.randn(10,1,10)  # 输入的shape(seq_len,batch_size,input_size)
        hidden = model.init_hidden(1)
        output,hidden,cn = model(input,hidden)
        print(output)   #输出的shape(seq_len,batch_size,hidden_size)  每一个时间步的最后一个隐藏层输出状态的集合 天气预报
        print(hidden)  #输出的shape(num_layers,batch_size,hidden_size) 最后一个循环层上所有隐藏层的状态  句子的情感
        print(cn)     #输出的shape(num_layers,batch_size,hidden_size) 最后一个循环层上所有细胞的状态  
```

![output](1.png)

![hn&cn](2.png)

LSTM网络的缺点： 
1. 计算复杂
2. 参数多，容易过拟合
3. 长期依赖问题仍然存在
4. 不易解释




