---
layout: mypost
title: 深度学习基本知识
categories: [深度学习]
extMath: true
---

## 1. 激活函数  
### 1.定义  
激活函数是神经网络的必要组成部分，数据经过函数运算后由激活函数映射输出。如果没有激活函数，多次线性运算的堆叠仍然是一个线性运算。如下图，Y的表达式f，就是一个激活函数。也就是说，每一个输入都有自己的权重，权重和输入的值相乘，然后加上一个偏置b之后在经过一个函数f得到输出y，这个f就是激活函数。

![激活函数](activation.png)

### 2.作用  
- 可以引入非线性因素，方便学习复杂数据。
- 激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。

### 3.常见的激活函数

#### 1.sigmoid函数

输入一个实值，输出一个 0 至 1 间的值。

![sigmoid](sigmoid.png)

![sigmoid1](sigmoid1.png)



**优点：**
1、 输出映射在(0,1)之间，单调连续，输出范围有限，优化稳定，可用作输出层；  
2、求导容易。  

**缺点：**  
1、易造成梯度消失； (当输入数值较大或较小时，梯度几乎为零)

2、输出非0均值，收敛慢；   (W_new = W_old - learning_rate * gradient, 梯度下降时，每次更新都会减去一个均值，导致收敛速度慢)

3、幂运算复杂，训练时间长。  

**应用：**  
1、二分类的多层感知机（MLP）的输出层；  
2、多层感知机（MLP）的隐藏层。


#### 2.tanh函数

输入一个实值，输出一个 -1 至 1 间的值。

![tanh](tanh.png)

![tanh1](tanh1.png)


**优点：**
1、比sigmoid函数收敛速度更快。
2、相比sigmoid函数，其输出以0为中心。

**缺点：**
1、易造成梯度消失；
2、幂运算复杂，训练时间长。

**应用：**
1、多层感知机（MLP）的隐藏层。


#### 3.ReLU函数

输入一个实值，输出一个 0 或正数。

![relu](ReLu.png)

![relu1](ReLu1.png)


**优点：**
1、解决了梯度消失问题(在正区间)；  
2、只需判断输入是否大于0，计算速度快；  
3、收敛速度远快于sigmoid和tanh，因为sigmoid和tanh涉及很多expensive的操作；  
4、提供了神经网络的稀疏表达能力  

**缺点：**
1、在负区间，输出恒为0，可能会造成神经元“死亡”。
2、输出非0均值，收敛慢；




                        
原文链接：https://blog.csdn.net/QLeelq/article/details/120931748

[https://blog.csdn.net/in546/article/details/119621649](https://blog.csdn.net/in546/article/details/119621649)

sigmoid，tanh，ReLU，Leaky ReLU，PReLU，RReLU， ELU（Exponential Linear Units），softplus，softsign，softmax

