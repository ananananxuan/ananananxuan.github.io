---
layout: mypost
title: NLP
categories: [深度学习]
extMath: true
---

### 1.自然语言处理数据

#### 1.1 文本数据

词与词，句子与句子，段落与段落之间有顺序

![数据样子](vector.png)
![数据样子](vectors.png)
在中文文字数据中，一张二维表是一个句子或者一段话，用词向量表示
(sequence_length,embedding_dimension)

(batch_size,sequence_length,embedding_dimension)

##### 分词
将一段文本分割成词语

1.中文分词   
jieba分词  
SNLP  

2.英文分词    
spacy分词  
BPE  



#### 1.2 时间序列数据

股票、气温记录、心电图

![数据样子](timeseries.png)

一张二维表单(time_step,feature_dimension)
多张二维表单(batch_size,time_step,feature_dimension)

#### 1.3 音频数据

#### 1.4 视频数据

……DNA序列数据
 
### 2.循环神经网络RNN
recurrent neural network
MLP处理方法
![MLP处理方法](rnn_word.png)
RNN处理方法
![RNN处理方法](rnn_word2.png)
![RNN处理方法](rnn_word3.png)

划分了几个词就进行几次正向传播，正向传播的次数称为时间步

将处理第一个字的隐藏层的信息传给处理下一个字的隐藏层

![RNN](RNN1.png)


rnn循环神经网络输入的是所有表的第一行
![deal](deal.png)

词嵌入 Embedding Layers


RNN网络： 

```
import torch
import torch.nn as nn

class RNN(nn.Module):
    def __init__(self,input_size,hidden_size,output_size):
        super(RNN,self).__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size,hidden_size) #input_size=input dimension,即词向量长度。hidden_size自己手动设置
        self.fc = nn.Linear(hidden_size,output_size)
    def forward(self,input,hidden):
        output,hidden = self.rnn(input,hidden)
        output = self.fc(output[-1])
        return output,hidden
    def init_hidden(self,batch_size):
        return torch.zeros(1,batch_size,self.hidden_size)
```
        
```
input = torch.randn(10,1,10)  # 输入的shape(seq_len,batch_size,input_size)
hidden = model.init_hidden(1)
output,hidden = model(input,hidden)
print(output)   #输出的shape(seq_len,batch_size,hidden_size)  每一个时间步的最后一个隐藏层输出状态的集合 天气预报
print(hidden)  #输出的shape(num_layers,batch_size,hidden_size) 最后一个循环层上所有隐藏层的状态  句子的情感
```
    
`




