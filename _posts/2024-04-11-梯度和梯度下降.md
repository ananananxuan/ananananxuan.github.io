---
layout: mypost
title: 梯度和梯度下降
categories: [深度学习]
extMath: true
---


## 梯度
### 1.定义
梯度（Gradient）：梯度是一个向量，表示某一函数在该点处的方向导数沿各个坐标轴的大小。

在多变量函数中，梯度是偏导数的向量。梯度的方向是函数在该点变化最快（增加最快）的方向，梯度的大小是函数在该点变化最快方向的变化率。例如在三维空间中，函数f(x, y, z) 在点(x0, y0, z0)处的梯度是一个向量，由函数f在该点关于x, y, z的偏导数组成，记作∇f或grad f。

## 梯度下降
梯度下降（Gradient Descent）：梯度下降是一种优化算法，主要用于寻找最小化目标函数（如损失函数）的参数。

它的基本思想是：在每一步迭代过程中，按照目标函数下降最快的方向（即梯度的负方向）更新参数。直观地看，梯度下降就像是从山顶走到山谷，每一步都沿着山坡下降最快的方向。在函数达到最低点（即函数值不再显著下降）时停止。

基于数据量大小和计算资源的不同，梯度下降算法可以分为批量梯度下降（Batch Gradient Descent），随机梯度下降（Stochastic Gradient Descent）及小批量梯度下降（Mini-Batch Gradient Descent）。

![梯度下降](loss_function.png)


## 优化器
### 1.定义
优化器就是在深度学习反向传播过程中，指引损失函数的各个参数往正确的方向更新合适的大小，使得更新后的各个参数让损失函数值不断逼近全局最小。

### 2.原理
梯度下降

### 3.常用优化器
1.SGD（随机梯度下降）  
每次只对一部分样本进行计算。计算速度快，但容易出现震荡。  

2.Momentum  
动量可以看作是为SGD增加了速度，帮助优化算法能在相关方向持续前进，抑制震荡，从而加快收敛。


3.Adam  
Momentum和RMSProp结合在一起，既考虑过去的梯度，又考虑过去的梯度平方。

4.Adagrad（自适应学习率算法） 
SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的embedding）。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。


### 4.如何选择合适的优化器
#### 1.根据模型复杂性
- 模型比较简单，例如线性模型或者较小的神经网络，那么传统的优化器，选择随机梯度下降(SGD)或者带有动量(momentum)的SGD，

- 模型较为复杂，如深度神经网络，那么Adam，RMSprop等自适应优化器可能会有更好的性能。

#### 2.根据数据特性
- 数据比较稀疏，选择自适应学习率的优化器，如Adam，Adagrad等，效果会更好。

- 数据比较密集，选择SGD，效果会更好。

#### 3.根据训练速度与计算资源
- 训练速度慢，选择自适应学习率的优化器，如Adam，Adagrad等，效果会更好。

- 训练速度快，选择SGD，效果会更好。

#### 4.根据过拟合风险
- 模型容易过拟合，选择SGD

- 模型不容易过拟合，选择Adam



