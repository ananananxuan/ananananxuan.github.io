---
layout: mypost
title: 爬虫
categories: [数据处理]
extMath: true
---

## 爬虫

### 1.定义

爬虫（crawler）是一种自动提取网页信息的程序，通过模拟浏览器的行为（如发送HTTP请求、解析HTML页面等），从互联网上获取所需的信息。

原则上，浏览器能做的事情，爬虫都能做

### 2.分类

1. 通用爬虫：根据预定的规则，自动从互联网上获取信息，并存储到本地或进行其他处理。
2. 聚焦爬虫：根据特定的需求，从互联网上获取所需的信息。

### 3.爬虫的原理

爬虫的原理就是模拟浏览器发送HTTP请求，并解析返回的HTML页面，从中提取所需的信息。

爬虫的实现需要用到网络爬虫框架（如Scrapy、BeautifulSoup等），以及相关的数据处理库（如pandas、numpy等）。

### 4.爬虫的作用

- 数据采集
- 软件测试
- 抢票

### 4.爬虫的流程

1. 确定目标：确定需要爬取的网站或网页（做一个url list），并确定需要提取的信息。
2. 发送请求：使用网络爬虫框架发送HTTP请求，获取网页的HTML代码。
3. 解析HTML：使用HTML解析库（如BeautifulSoup）解析HTML代码，提取所需的信息。
4. 存储数据：将提取的信息存储到本地文件或数据库中。
![爬虫的流程图](spider.png)

### 5.爬虫的注意点

1. 遵守网站的robots.txt协议，避免被网站封禁。
2. 避免频繁访问同一网站，避免给网站带来过大的压力。


爬虫架构

![爬虫架构](webcrawler.png)

### 6.爬虫的框架

1. Scrapy：一个Python编写的爬虫框架，支持异步处理，具有良好的扩展性和可定制性。
2. BeautifulSoup：一个Python编写的HTML解析库，支持多种解析方式，具有良好的易用性和扩展性。
3. Requests：一个Python编写的HTTP请求库，支持多种请求方法，具有良好的易用性和扩展性。

### requests库
用于网页下载  
![](requests.png)
![](requests_response.png)

```
import requests
url = "http://www.crazyant.net/"

r = requests.get(url)
r.status_code  #返回200表示请求成功
r.headers #关注'Content-Type':是否设置charset为'UTF-8'
r.encoding #关注是否为"UTF-8"
r.text #关注meta中charset是否为"UTF-8"，返回具体内容
r.cookies

```

### url管理器
基本逻辑
![](url_manage.png)



```
class UrlManager():
  def __init__(self):
    self.new_urls = set()
    self.old_urls = set() 

  def add_new_url(self, url):
    if url is None or len(url) == 0:
      return
    if url not in self.new_urls and url not in self.old_urls:
      self.new_urls.add(url)

  def add_new_urls(self, urls):
    if urls is None or len(urls) == 0:
      return
    for url in urls:
      self.add_new_url(url)

  def get_url(self):
    if self.has_new_url():
      url = self.new_urls.pop()
      self.old_urls.add(url)
      return url
    else:
      return None
    
  def has_new_url(self):
    return len(self.new_urls) > 0

```
 
```  
if __name__ == '__main__':
    url_manager = UrlManager()

    url_manager.add_new_url('http://www.baidu.com')
    url_manager.add_new_urls(['http://www.baidu.com',"http://www.crazyant.net/"])
    print(url_manager.new_urls,url_manager.old_urls)

    print("#"*40)
    new_url = url_manager.get_url()
    print(url_manager.new_urls,url_manager.old_urls)

    print("#"*40)
    new_url = url_manager.get_url()
    print(url_manager.new_urls,url_manager.old_urls)

    print("#"*40)
    print(url_manager.has_new_url())
```
### BeautifulSoup库
用于从HTML中提取数据

![](bs4.png)

具体方法：
![](bs4find.png)

![](bs4find2.png)


