---
layout: mypost
title: BERT
categories: [深度学习]
---

BERT（Bidirectional Encoder Representations from Transformers）是由Google在2018年提出的一种预训练语言模型。BERT的创新之处在于，它采用了双向Transformer编码器来生成上下文相关的词向量表示。
传统的单向语言模型只考虑了左侧或右侧的上下文信息，而BERT则同时考虑了左侧和右侧的上下文信息，使得生成的词向量具有更好的语义表达能力。