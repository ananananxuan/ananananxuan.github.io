---
layout: mypost
title: 深度学习基本知识
categories: [深度学习]
extMath: true
---

## 1. 激活函数  
### 1.定义  
激活函数是神经网络的必要组成部分，数据经过函数运算后由激活函数映射输出。如果没有激活函数，多次线性运算的堆叠仍然是一个线性运算。如下图，Y的表达式f，就是一个激活函数。也就是说，每一个输入都有自己的权重，权重和输入的值相乘，然后加上一个偏置b之后在经过一个函数f得到输出y，这个f就是激活函数。

![激活函数](activation.png)

### 2.满足条件

- 非线性： 激活函数非线性时，多层神经网络可逼近所有函数。
- 可微性： 优化器大多用梯度下降更新参数。
- 单调性： 当激活函数是单调的，能保证单层网络的损失函数是凸函数。
- 近似恒等性：当参数初始化为随机小值时，神经网络更稳定。

### 3.作用  
- 可以引入非线性因素，方便学习复杂数据。
- 激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。

### 4.常见的激活函数

#### 1.sigmoid函数

输入一个实值，输出一个 0 至 1 间的值。

![sigmoid](sigmoid.png)

![sigmoid1](sigmoid1.png)



**优点：**  
1、 输出映射在(0,1)之间，单调连续，输出范围有限，优化稳定，可用作输出层；  
2、求导容易。   

**缺点：**  
1、易造成梯度消失； (当输入数值较大或较小时，梯度几乎为零)

2、输出非0均值，收敛慢；   (W_new = W_old - learning_rate * gradient, 梯度下降时，每次更新都会减去一个均值，导致收敛速度慢)

3、幂运算复杂，训练时间长。  

**应用：**  
1、二分类的多层感知机（MLP）的输出层；  
2、多层感知机（MLP）的隐藏层。


#### 2.tanh函数

输入一个实值，输出一个 -1 至 1 间的值。

![tanh](tanh.png)

![tanh1](tan1.png)


**优点：**     
1、比sigmoid函数收敛速度更快。  
2、相比sigmoid函数，其输出以0为中心。  

**缺点：**    
1、易造成梯度消失；  
2、幂运算复杂，训练时间长。  

**应用：**   
1、多层感知机（MLP）的隐藏层。  
2、多层感知机（MLP）的输出层。  


#### 3.ReLU函数

输入一个实值，输出一个 0 或正数。

![relu](ReLu.png)

![relu1](ReLu1.png)


**优点：**  
1、解决了梯度消失问题(在正区间)；  
2、只需判断输入是否大于0，计算速度快；  
3、收敛速度远快于sigmoid和tanh，因为sigmoid和tanh涉及很多expensive的操作；  
4、提供了神经网络的稀疏表达能力  

**缺点：**  
1、在负区间，输出恒为0，可能会造成神经元“死亡”。
2、输出非0均值，收敛慢；

**应用：**  
1、多层感知机（MLP）的隐藏层。

#### 4.Leaky ReLU函数

输入一个实值，输出一个 0 或正数。

![Leakyrelu](Leak Relu.png)

![Leakyrelu1](Leak Relu1.png)


**优点：**  
1、解决了梯度消失问题(在正区间)；  
2、只需判断输入是否大于0，计算速度快；  

**缺点：**  
理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。

**应用：**  
1、多层感知机（MLP）的隐藏层。

#### 5.softmax函数

输入一个实值，输出一个 0 至 1 间的值。

![softmax](softmax.png)



**优点：**  
对神经网络全连接层输出进行变换，使其服从概率分布，即每个值都位于[0,1]区间且和为1。

**缺点：**  
1、计算复杂，训练时间长。

**应用：**  
1、多分类问题，多层感知机（MLP）的输出层。
            
参考链接：

[https://blog.csdn.net/QLeelq/article/details/120931748](https://blog.csdn.net/QLeelq/article/details/120931748)

[https://blog.csdn.net/in546/article/details/119621649](https://blog.csdn.net/in546/article/details/119621649)

## 2.损失函数  

### 1.定义
损失函数（Loss function）是用于评估模型预测与真实值差距的度量。

### 2.作用
损失函数是用来评估模型预测结果与真实结果的差异性，从而指导模型进行训练。

### 3.常见的损失函数

#### 1.均方误差（MSE）
最为常见的损失函数，主用于回归问题。MSE计算的是预测值与真实值之间差值的平方的平均值。

![MSE](MSE.png)

```
import torch
import torch.nn as nn

loss = nn.MSELoss()
# 假设input和target分别为预测值和真实值
loss_value = loss(input, target)
``` 


#### 2.交叉熵损失函数（Cross Entropy Loss）

交叉熵损失主要用于分类问题，尤其是在二分类（如Sigmoid交叉熵）和多分类（如Softmax交叉熵）任务中。它对预测概率分布与实际概率分布之间的差距进行衡量。交叉熵损失nn.CrossEntropyLoss()在内部执行了Log Softmax操作，因此，它的输入应当是未经Softmax的原始分数（logits）。


```
    loss = nn.CrossEntropyLoss()
    # input是一个矩阵，每行表示一个样本的预测类别分数，
    # target是一个向量，表示每个样本的真实类别
    loss_value = loss(input, target)
```


#### 3.平均绝对误差（MAE）

与MSE类似，MAE计算的是预测值与真实值的绝对差的平均值。相比于MSE，MAE对异常值具有更强的鲁棒性。

```
loss = nn.L1Loss()
loss_value = loss(input, target)
```


#### 4.Huber损失函数

Huber损失是均方误差和平均绝对误差的结合。它在预测误差较小时表现如MSE，在预测误差较大时表现如MAE，兼具两者的优点。

```
loss = nn.SmoothL1Loss()
loss_value = loss(input, target)
```

#### 5.对数损失

对数损失是交叉熵损失的另一种常见形式，主要用于二分类问题。它考虑的是预测概率本身的对数值，而非简单的预测正确与否。在PyTorch中，可以使用BCELoss(Binary Cross Entropy Loss)替代对数损失，主要用于二分类问题。

```
    loss = nn.BCELoss()
    loss_value = loss(input, target)
```

#### 6.Hinge损失

Hinge损失常用于支持向量机（SVM）和一些感知机算法，但也可以用于深度学习模型中，特别是对于一些"最大-最小"类别判别的问题。

```
    loss = nn.HingeEmbeddingLoss()
    loss_value = loss(input, target)
```


